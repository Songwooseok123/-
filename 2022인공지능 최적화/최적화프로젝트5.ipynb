{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3865,
     "status": "ok",
     "timestamp": 1668788684021,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "qlRolimBoaxb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import joblib\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier,BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.svm as svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "train_data= pd.read_excel('train.xlsx',engine='openpyxl')\n",
    "test_data = pd.read_excel('test.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient number</th>\n",
       "      <th>Prediction day</th>\n",
       "      <th>성별</th>\n",
       "      <th>나이</th>\n",
       "      <th>입원일자</th>\n",
       "      <th>수술일자</th>\n",
       "      <th>퇴원일자</th>\n",
       "      <th>신장(cm)</th>\n",
       "      <th>체중(kg)</th>\n",
       "      <th>BMI</th>\n",
       "      <th>...</th>\n",
       "      <th>JP_Amy_Lt_POD#1</th>\n",
       "      <th>JP_Amy_Lt_POD#2</th>\n",
       "      <th>JP_Amy_Lt_POD#3</th>\n",
       "      <th>JP_Amy_Lt_POD#5</th>\n",
       "      <th>JP_Lt_color change</th>\n",
       "      <th>JP_Lip_Lt_POD#2</th>\n",
       "      <th>JP_Lip_Lt_POD#3</th>\n",
       "      <th>JP_Lip_Lt_POD#5</th>\n",
       "      <th>DSL</th>\n",
       "      <th>onset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>2019-01-02 00:00:00</td>\n",
       "      <td>2019-01-08 00:00:00</td>\n",
       "      <td>177.8</td>\n",
       "      <td>54.1</td>\n",
       "      <td>17.11</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>2019-01-02 00:00:00</td>\n",
       "      <td>2019-01-08 00:00:00</td>\n",
       "      <td>173.9</td>\n",
       "      <td>72.6</td>\n",
       "      <td>24.01</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>2019-01-02 00:00:00</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01-09 00:00:00</td>\n",
       "      <td>157.2</td>\n",
       "      <td>73.4</td>\n",
       "      <td>29.7</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>2019-01-03 00:00:00</td>\n",
       "      <td>2019-01-04 00:00:00</td>\n",
       "      <td>2019-01-23 00:00:00</td>\n",
       "      <td>168.9</td>\n",
       "      <td>71.5</td>\n",
       "      <td>25.06</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>2019-01-06 00:00:00</td>\n",
       "      <td>2019-01-07 00:00:00</td>\n",
       "      <td>2019-01-15 00:00:00</td>\n",
       "      <td>165.1</td>\n",
       "      <td>76</td>\n",
       "      <td>27.88</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>2020-12-16 00:00:00</td>\n",
       "      <td>2020-12-17 00:00:00</td>\n",
       "      <td>2020-12-24 00:00:00</td>\n",
       "      <td>164.9</td>\n",
       "      <td>66.9</td>\n",
       "      <td>24.6</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>2020-12-22 00:00:00</td>\n",
       "      <td>2020-12-23 00:00:00</td>\n",
       "      <td>2020-12-29 00:00:00</td>\n",
       "      <td>149</td>\n",
       "      <td>47.7</td>\n",
       "      <td>21.49</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>2020-12-23 00:00:00</td>\n",
       "      <td>2020-12-24 00:00:00</td>\n",
       "      <td>2020-12-30 00:00:00</td>\n",
       "      <td>166.1</td>\n",
       "      <td>71.4</td>\n",
       "      <td>25.88</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>2020-12-28 00:00:00</td>\n",
       "      <td>2020-12-30 00:00:00</td>\n",
       "      <td>2021-01-08 00:00:00</td>\n",
       "      <td>154.7</td>\n",
       "      <td>47.8</td>\n",
       "      <td>19.97</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>2020-12-30 00:00:00</td>\n",
       "      <td>2020-12-31 00:00:00</td>\n",
       "      <td>2021-01-08 00:00:00</td>\n",
       "      <td>168.3</td>\n",
       "      <td>53.5</td>\n",
       "      <td>18.89</td>\n",
       "      <td>...</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Patient number  Prediction day  성별  나이                 입원일자  \\\n",
       "0                 1             NaN   1  53  2019-01-01 00:00:00   \n",
       "1                 2             NaN   1  62  2019-01-01 00:00:00   \n",
       "2                 3             NaN   0  70  2019-01-02 00:00:00   \n",
       "3                 4             NaN   1  72  2019-01-03 00:00:00   \n",
       "4                 5             NaN   1  53  2019-01-06 00:00:00   \n",
       "..              ...             ...  ..  ..                  ...   \n",
       "540             225             NaN   1  68  2020-12-16 00:00:00   \n",
       "541             226             NaN   0  63  2020-12-22 00:00:00   \n",
       "542             227             NaN   1  68  2020-12-23 00:00:00   \n",
       "543             228             NaN   0  59  2020-12-28 00:00:00   \n",
       "544             229             NaN   0  50  2020-12-30 00:00:00   \n",
       "\n",
       "                    수술일자                 퇴원일자 신장(cm) 체중(kg)    BMI  ...  \\\n",
       "0    2019-01-02 00:00:00  2019-01-08 00:00:00  177.8   54.1  17.11  ...   \n",
       "1    2019-01-02 00:00:00  2019-01-08 00:00:00  173.9   72.6  24.01  ...   \n",
       "2    2019-01-03 00:00:00  2019-01-09 00:00:00  157.2   73.4   29.7  ...   \n",
       "3    2019-01-04 00:00:00  2019-01-23 00:00:00  168.9   71.5  25.06  ...   \n",
       "4    2019-01-07 00:00:00  2019-01-15 00:00:00  165.1     76  27.88  ...   \n",
       "..                   ...                  ...    ...    ...    ...  ...   \n",
       "540  2020-12-17 00:00:00  2020-12-24 00:00:00  164.9   66.9   24.6  ...   \n",
       "541  2020-12-23 00:00:00  2020-12-29 00:00:00    149   47.7  21.49  ...   \n",
       "542  2020-12-24 00:00:00  2020-12-30 00:00:00  166.1   71.4  25.88  ...   \n",
       "543  2020-12-30 00:00:00  2021-01-08 00:00:00  154.7   47.8  19.97  ...   \n",
       "544  2020-12-31 00:00:00  2021-01-08 00:00:00  168.3   53.5  18.89  ...   \n",
       "\n",
       "    JP_Amy_Lt_POD#1 JP_Amy_Lt_POD#2 JP_Amy_Lt_POD#3 JP_Amy_Lt_POD#5  \\\n",
       "0                 *               *               *               *   \n",
       "1                 *               *               *               *   \n",
       "2                 *               *               *               *   \n",
       "3                 *               *               *               *   \n",
       "4                 *               *               *               *   \n",
       "..              ...             ...             ...             ...   \n",
       "540               *               *               *               *   \n",
       "541               *               *               *               *   \n",
       "542               *               *               *               *   \n",
       "543               *               *               *               *   \n",
       "544               *               *               *               *   \n",
       "\n",
       "    JP_Lt_color change JP_Lip_Lt_POD#2 JP_Lip_Lt_POD#3 JP_Lip_Lt_POD#5 DSL  \\\n",
       "0                    *               *               *               *   *   \n",
       "1                    *               *               *               *   *   \n",
       "2                    *               *               *               *   *   \n",
       "3                    *               *               *               *   *   \n",
       "4                    *               *               *               *   *   \n",
       "..                 ...             ...             ...             ...  ..   \n",
       "540                  *               *               *               *   *   \n",
       "541                  *               *               *               *   *   \n",
       "542                  *               *               *               *   *   \n",
       "543                  *               *               *               *   *   \n",
       "544                  *               *               *               *   *   \n",
       "\n",
       "    onset  \n",
       "0       *  \n",
       "1       *  \n",
       "2       *  \n",
       "3       *  \n",
       "4       *  \n",
       "..    ...  \n",
       "540     *  \n",
       "541     *  \n",
       "542     *  \n",
       "543     *  \n",
       "544     *  \n",
       "\n",
       "[545 rows x 140 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668788684022,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "4GC_cDKfpIyX"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pre_processing(train_data):\n",
    "    \n",
    "    # 필요없는 column 제거\n",
    "    ## train시 모두 *인 컬럼 :  '타장기원발암', 'JP_Lt_color change'\n",
    "    ## Clipping , 수술 전 EUS 0.2 , 수술 전 PET-CT , TNM ,CEA(ng/mL) ,CA19-9(U/mL) cTNM, post ESD 의사쌤이 안넣어도 된다함.  \n",
    "    ## 입원일자, 퇴원일자 없애고 , 수술일자만 카테고리 나눠서 쓸것임. \n",
    "    train_data=train_data.drop(['JP_Lt_color change','입원일자','퇴원일자','Patient number','타장기원발암','Clipping ',\n",
    "                                '수술 전 EUS','수술 전 PET-CT','TNM','CEA(ng/mL)','CA19-9(U/mL)','cTNM',\n",
    "                                'post ESD'],axis =1 ) \n",
    "    ## 'DSL'에 언젠가 걸릴지 예측하는 문제 -> onset은 언제걸렷는지 안중요해서 필요없을듯\n",
    "    if 'onset' in list(train_data.columns):\n",
    "      train_data=train_data.drop(['onset'],axis =1 )\n",
    "    \n",
    "    \n",
    "    # 필요한 column 추가\n",
    "    ## margin(p)', 'margin(d)'는 그 사람의 병변의 갯수만큼 값을 가지고 있음. 따라서 병변2,병변3 처럼 margin2,margin3 추가해줌. \n",
    "    train_data.rename(columns ={'margin(p)':'marginP','margin(d)':'marginD'},inplace = True)\n",
    "\n",
    "    train_data['marginP2'] = train_data.marginP.str.split('/').str[1]\n",
    "    train_data['marginP3'] = train_data.marginP.str.split('/').str[2]\n",
    "    train_data['marginP2']=train_data['marginP2'].fillna('*')\n",
    "    train_data['marginP3']=train_data['marginP3'].fillna('*')\n",
    "\n",
    "    train_data['marginD2'] = train_data.marginD.str.split('/').str[1]\n",
    "    train_data['marginD3'] = train_data.marginD.str.split('/').str[2]\n",
    "    train_data['marginD2']=train_data['marginD2'].fillna('*')\n",
    "    train_data['marginD3']=train_data['marginD3'].fillna('*')\n",
    "\n",
    "    #결측치 너무 많은 row 제거(70% 이상) : DSL 안 걸린 사람 7명 없어짐  \n",
    "    train_data = train_data.loc[(train_data =='*').mean(axis=1)<0.7]\n",
    "    train_data=train_data.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # '수술일자' -> 수술일자 날씨 카테고리로 변경(봄0,여름1,가을2,겨울3)\n",
    "    for i in range(len(train_data)):\n",
    "        if train_data.loc[i,'수술일자'] != '*' :\n",
    "          train_data.loc[i,'수술일자']= int(train_data.loc[i,'수술일자'].strftime(\"%m\"))\n",
    "        else:\n",
    "          train_data.loc[i,'수술일자']= 7\n",
    "\n",
    "        if (train_data.loc[i,'수술일자'] ==3) or (train_data.loc[i,'수술일자'] ==4) or (train_data.loc[i,'수술일자'] ==5):\n",
    "            train_data.loc[i,'수술일자'] = 0\n",
    "        elif train_data.loc[i,'수술일자'] ==6 or (train_data.loc[i,'수술일자'] ==7) or (train_data.loc[i,'수술일자'] ==8):\n",
    "            train_data.loc[i,'수술일자'] = 1\n",
    "\n",
    "        elif train_data.loc[i,'수술일자'] ==9 or (train_data.loc[i,'수술일자'] ==10) or (train_data.loc[i,'수술일자'] ==11):\n",
    "            train_data.loc[i,'수술일자'] = 2\n",
    "\n",
    "        elif train_data.loc[i,'수술일자'] ==12 or (train_data.loc[i,'수술일자'] ==1) or (train_data.loc[i,'수술일자'] ==2):\n",
    "            train_data.loc[i,'수술일자'] = 3\n",
    "        \n",
    "        # 위암 병소수 : 병변의 갯수임. 따라서 0은 오타고, 병변의 갯수 만큼(대부분 1) 채워넣으면 됨->  0인 애들,결측치 * 인애들 다 1로 바꿈 \n",
    "        if (train_data.loc[i,'위암병소수'] == '*') or (train_data.loc[i,'위암병소수'] == 0) : \n",
    "            train_data.loc[i,'위암병소수'] = 1\n",
    "        \n",
    "        # '병변1 AGC 분류' 와 '병변1 EGC 분류' 둘 중 하나는 꼭 있어야 되는 feature임. (육안상 진행성 위암인지 조기위암인지) \n",
    "        #   -> 둘다 결측치라면 현미경으로 관측한 정보 'AGC 분류'와 'EGC 분류'로 채워넣음.  \n",
    "        if (train_data.loc[i,'병변1 AGC 분류'] == '*') and (train_data.loc[i,'병변1 EGC 분류'] == '*') :\n",
    "            train_data.loc[i,'병변1 AGC 분류'] = train_data.loc[i,'AGC 분류'] \n",
    "            train_data.loc[i,'병변1 EGC 분류'] = train_data.loc[i,'EGC 분류']\n",
    "\n",
    "        # 'tubular 위치' 컬럼과 '병변1 tubular'의 카테고리를 같게 통일해줌 . (ex. 0: upper)\n",
    "        if (train_data.loc[i,'tubular 위치'] == 1) :\n",
    "            train_data.loc[i,'tubular 위치'] = 0\n",
    "        elif (train_data.loc[i,'tubular 위치'] == 2) :\n",
    "          train_data.loc[i,'tubular 위치'] = 1\n",
    "        elif (train_data.loc[i,'tubular 위치'] == 3) or (train_data.loc[i,'tubular 위치'] == 4) or (train_data.loc[i,'tubular 위치'] == 5) or (train_data.loc[i,'tubular 위치'] == 6) :\n",
    "          train_data.loc[i,'tubular 위치'] = 2\n",
    "        elif (train_data.loc[i,'tubular 위치'] == 7) :\n",
    "          train_data.loc[i,'tubular 위치'] = 3\n",
    "        elif len(str(train_data.loc[i,'tubular 위치'])) != 1 :\n",
    "          train_data.loc[i,'tubular 위치'] = 2 \n",
    "\n",
    "        # 현미경으로 관측한 정보 'tubular 위치'와 'circular 위치'로  '병변1 tubular'와 '병변1 circular'를 채워줌 \n",
    "        if (train_data.loc[i,'병변1 tubular'] == '*') :\n",
    "          train_data.loc[i,'병변1 tubular'] = train_data.loc[i,'tubular 위치']\n",
    "        else : train_data.loc[i,'병변1 tubular'] = int(train_data.loc[i,'병변1 tubular'])\n",
    "        if (train_data.loc[i,'병변1 circular'] == '*') :\n",
    "          train_data.loc[i,'병변1 circular'] = train_data.loc[i,'circular 위치']\n",
    "\n",
    "        # 위에 marginP2,marginD2,marginD3,marginD3는 업데이트 했었음. marginP 업데이트(값 1개만 남게)\n",
    "        if type(train_data.loc[i,'marginP']) == str:\n",
    "          if train_data.loc[i,'marginP'] != '*' :\n",
    "            train_data.loc[i,'marginP']= float(train_data.loc[i,'marginP'].split('/')[0])\n",
    "        if type(train_data.loc[i,'marginD']) == str:\n",
    "          if train_data.loc[i,'marginD'] != '*' :\n",
    "            train_data.loc[i,'marginD']= float(train_data.loc[i,'marginD'].split('/')[0])\n",
    "\n",
    "        # '*'이 결측치가 아니라 '없음'이라는 카테고리인 경울 -> 이미 있는 '없음 카테고리' 0으로 바꿔줌  \n",
    "        if (train_data.loc[i,'가족암병력'] == '*') :\n",
    "          train_data.loc[i,'가족암병력'] = 0\n",
    "        if (train_data.loc[i,'DSL'] == '*') :\n",
    "          train_data.loc[i,'DSL'] = 0   \n",
    "\n",
    "        # 0이 아니라 결측값인 애들 -> '*'로 바꿔줌    \n",
    "        if (train_data.loc[i,'AGC 분류'] == 0) :\n",
    "          train_data.loc[i,'AGC 분류'] = '*'\n",
    "        if (train_data.loc[i,'EGC 분류'] == 0) :\n",
    "          train_data.loc[i,'EGC 분류'] = '*'   \n",
    "    \n",
    "    ################# Numerical data #################\n",
    "    # Numerical data imputation -> 평균으로 대체\n",
    "    Ordinal_data = ['ASA score','LN dissection ','위암병소수' ] \n",
    "    Numerical_data = ['나이','신장(cm)', '체중(kg)','BMI','수술시간(min)','출혈량(mL)','GAS OUT','SD start',\n",
    "                      '병변1 크기(Cm)','marginP','marginD','구득 림프절수','전이 림프절수'] +Ordinal_data\n",
    "    \n",
    "    for numeric in Numerical_data :\n",
    "      train_data[numeric] =  train_data[numeric].replace('*',np.NaN)\n",
    "      train_data[numeric] = pd.to_numeric(train_data[numeric])\n",
    "      train_nu_mean=train_data[numeric].mean()\n",
    "      train_data[numeric]= train_data[numeric].fillna(train_data[numeric].mean())\n",
    "    ## bmi 는 (체중*10000) / (신장**2)\n",
    "    train_data['BMI'] = (train_data['체중(kg)']*10000)/(train_data['신장(cm)']**2)\n",
    "\n",
    "    # numerical data 인데*이 결측치가 아니라 측정 안한거: '*'을 0으로 바꾸면 됨. \n",
    "\n",
    "    star_is_zero_numeric = [ '병변2 크기(Cm)', '병변3 크기(Cm)','marginP2','marginD2','marginP3','marginD3',\n",
    "                               'WBC_Pre', 'WBC_Post', 'WBC_POD#1', 'WBC_POD#2', 'WBC_POD#3', 'WBC_POD#5', 'WBC_POD#7',\n",
    "                               'Hb_Pre', 'Hb_Post', 'Hb_POD#1', 'Hb_POD#2', 'Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7', \n",
    "                               'AST_Pre', 'AST_Post', 'AST_POD#1', 'AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7', \n",
    "                               'ALT_Pre', 'ALT_Post', 'ALT_POD#1', 'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5', 'ALT_POD#7', \n",
    "                               'CRP_Pre', 'CRP_Post', 'CRP_POD#1', 'CRP_POD#2', 'CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "                               'JP_Amy_Rt_POD#1', 'JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5', \n",
    "                               'JP_Rt_color change', 'JP_Lip_Rt_POD#1', 'JP_Lip_Rt_POD#2', 'JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "                               'JP_Amy_Lt_POD#1', 'JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5', \n",
    "                               'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']\n",
    "    for i in range(len(train_data)):\n",
    "      for value in star_is_zero_numeric :\n",
    "        if (train_data.loc[i,value] == '*') :\n",
    "          train_data.loc[i,value] = 0      \n",
    "    \n",
    "    ################# Categorical data #################\n",
    "    \n",
    "    ## '*'을 그대로 놔둬야 하는 애들 -> '*'이라는 category를 주고 imputation 안함. \n",
    "    star_is_category = ['AGC 분류','EGC 분류','tubular 위치','circular 위치','병변1 AGC 분류','병변1 EGC 분류',\n",
    "                    '병변1 tubular', '병변1 circular','병변2 AGC 분류','병변2 EGC 분류',\n",
    "                    '병변2 tubular', '병변2 circular','병변3 AGC 분류','병변3 EGC 분류',\n",
    "                    '병변3 tubular', '병변3 circular' ]\n",
    "\n",
    "\n",
    "    ## imputation 해야 되는 애들 \n",
    "\n",
    "    ###  결측치가 10 % 미만인 컬럼들 -> 빈도수 가장 많은 걸로\n",
    "    train_data = train_data.rename(columns={'ESD/EMR': 'ESDEMR','정규/응급': '정규응급'})\n",
    "    Categorical_data_under10 =['성별','수술일자','smoking','가족암병력','HTN','DM','Dyslipidemia',\n",
    "                       '심혈관질환','뇌혈관질환','신장질환','호흡기질환','기타 기저질환','복부 수술력','복부수술력 중 위관련',\n",
    "                       '기타 수술력','수술 전   심초음파','수술 전   폐기능검사','수술 전    흉부CT검사',\n",
    "                       'ESDEMR','정규응급','수술명','개복전환','문합법','합병절제                   ',\n",
    "                       'adhesion','invasion','radicality','혈관변이',\n",
    "                       '수혈']\n",
    "    binbin =[]\n",
    "    for bin in Categorical_data_under10:\n",
    "      binbin.append(train_data[bin].value_counts().idxmax())\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "      for bin in Categorical_data_under10:\n",
    "        if train_data.loc[i,bin] == '*' :\n",
    "            train_data.loc[i,bin] = train_data[bin].value_counts().idxmax()\n",
    "    \n",
    "    ### 결측치가 10 % 이상인 컬럼들 -> random sample imputation :원래의 분포를 유지하도록 결측치를 imputation  \n",
    "    Categorical_data_over10 =['depth','Stage','WHO classification','WHO 세포 분화도',\"Lauren's classification\",\n",
    "                            'Ming classification','lymphatics invasion','vascular invasion','perineural invasion',\n",
    "                            'additional findings']    \n",
    "    for category10 in Categorical_data_over10 :\n",
    "      train_data['_random'] = train_data[category10]\n",
    "      train_data.loc[train_data[category10]!='*'][category10]\n",
    "      temp = (train_data.loc[train_data[category10]!='*'][category10].sample(  (train_data[category10]=='*').sum()  ,replace =True ))\n",
    "      temp.index = train_data[lambda x:  (x[category10]=='*')].index # index 부여 \n",
    "      train_data.loc[(train_data[category10]=='*'), '_random'] = temp\n",
    "      train_data[category10] = train_data['_random']\n",
    "      train_data=train_data.drop(['_random'],axis =1 )\n",
    "\n",
    "    ## Onehotencoding\n",
    "    CATEGROCAL = star_is_category+Categorical_data_under10+Categorical_data_over10\n",
    "    train_data[CATEGROCAL]= train_data[CATEGROCAL].astype(str)\n",
    "    train_data= pd.get_dummies(train_data, columns = CATEGROCAL)\n",
    "    \n",
    "    # EGC 분류,circular 위치,가족암병력, WHO classification, WHO 세포분화도,Lauren's classification,Ming classification 처럼\n",
    "    # 한 사람이 값을 2개 씩 가질 수 있는 컬럼 -> 원핫 인코딩하면 '가족암병력_2,3' 이런 컬럼이 추가됨. 이거 다시 나눠줌.  \n",
    "    remo =[]\n",
    "    for i in range(len(train_data)):\n",
    "      for col in train_data.columns:\n",
    "        if (',' in col) :\n",
    "          remo.append(col)\n",
    "          if (train_data.loc[i,col] == 1) :\n",
    "            splitted_col = col.split(',')\n",
    "            train_data.loc[i,splitted_col[0]] = 1\n",
    "            for j in range(1,len(splitted_col)):\n",
    "              train_data.loc[i,(splitted_col[0].split('_'))[0]+'_'+splitted_col[j] ] = 1\n",
    "        if ('/' in col) :\n",
    "          remo.append(col)\n",
    "          if (train_data.loc[i,col] == 1) :\n",
    "            splitted_col = col.split('/')\n",
    "            train_data.loc[i,splitted_col[0]] = 1     \n",
    "            for j in range(1,len(splitted_col)):\n",
    "              train_data.loc[i,(splitted_col[0].split('_'))[0]+'_'+splitted_col[j] ] = 1\n",
    "    a = list(set(remo))\n",
    "    train_data= train_data.drop(a,axis=1)    \n",
    "    \n",
    "    train_data = train_data.fillna(0)\n",
    "    train_data = train_data.astype('float')\n",
    "\n",
    "    return train_data,temp,train_nu_mean,binbin\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668788684023,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "_8G8wRioojOb"
   },
   "outputs": [],
   "source": [
    "def preprocess_test(test_data):\n",
    "  # 필요없는 column 제거\n",
    "  ## test시 모두 *인 컬럼 :  '타장기원발암', 'JP_Lt_color change'\n",
    "  ## Clipping , 수술 전 EUS 0.2 , 수술 전 PET-CT , TNM ,CEA(ng/mL) ,CA19-9(U/mL) cTNM, post ESD 의사쌤이 안넣어도 된다함.  \n",
    "  ## 입원일자, 퇴원일자 없애고 , 수술일자만 카테고리 나눠서 쓸것임. \n",
    "  test_data=test_data.drop(['JP_Lt_color change','입원일자','퇴원일자','Patient number','타장기원발암','Clipping ',\n",
    "                              '수술 전 EUS','수술 전 PET-CT','TNM','CEA(ng/mL)','CA19-9(U/mL)','cTNM',\n",
    "                              'post ESD'],axis =1 ) \n",
    "  ## 'DSL'에 언젠가 걸릴지 예측하는 문제 -> onset은 언제걸렷는지 안중요해서 필요없을듯\n",
    "  if 'onset' in list(test_data.columns):\n",
    "    test_data=test_data.drop(['onset'],axis =1 )\n",
    "\n",
    "\n",
    "  # 필요한 column 추가\n",
    "  ## margin(p)', 'margin(d)'는 그 사람의 병변의 갯수만큼 값을 가지고 있음. 따라서 병변2,병변3 처럼 margin2,margin3 추가해줌. \n",
    "  test_data.rename(columns ={'margin(p)':'marginP','margin(d)':'marginD'},inplace = True)\n",
    "\n",
    "  test_data['marginP2'] = test_data.marginP.str.split('/').str[1]\n",
    "  test_data['marginP3'] = test_data.marginP.str.split('/').str[2]\n",
    "  test_data['marginP2']=test_data['marginP2'].fillna('*')\n",
    "  test_data['marginP3']=test_data['marginP3'].fillna('*')\n",
    "\n",
    "  test_data['marginD2'] = test_data.marginD.str.split('/').str[1]\n",
    "  test_data['marginD3'] = test_data.marginD.str.split('/').str[2]\n",
    "  test_data['marginD2']=test_data['marginD2'].fillna('*')\n",
    "  test_data['marginD3']=test_data['marginD3'].fillna('*')\n",
    "\n",
    "  #결측치 너무 많은 row 제거(70% 이상) : DSL 안 걸린 사람 7명 없어짐  \n",
    "  test_data = test_data.loc[(test_data =='*').mean(axis=1)<0.7]\n",
    "  test_data=test_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "  # '수술일자' -> 수술일자 날씨 카테고리로 변경(봄0,여름1,가을2,겨울3)\n",
    "  for i in range(len(test_data)):\n",
    "      if test_data.loc[i,'수술일자'] != '*' :\n",
    "        test_data.loc[i,'수술일자']= int(test_data.loc[i,'수술일자'].strftime(\"%m\"))\n",
    "      else:\n",
    "        test_data.loc[i,'수술일자']= 7\n",
    "\n",
    "      if (test_data.loc[i,'수술일자'] ==3) or (test_data.loc[i,'수술일자'] ==4) or (test_data.loc[i,'수술일자'] ==5):\n",
    "          test_data.loc[i,'수술일자'] = 0\n",
    "      elif test_data.loc[i,'수술일자'] ==6 or (test_data.loc[i,'수술일자'] ==7) or (test_data.loc[i,'수술일자'] ==8):\n",
    "          test_data.loc[i,'수술일자'] = 1\n",
    "\n",
    "      elif test_data.loc[i,'수술일자'] ==9 or (test_data.loc[i,'수술일자'] ==10) or (test_data.loc[i,'수술일자'] ==11):\n",
    "          test_data.loc[i,'수술일자'] = 2\n",
    "\n",
    "      elif test_data.loc[i,'수술일자'] ==12 or (test_data.loc[i,'수술일자'] ==1) or (test_data.loc[i,'수술일자'] ==2):\n",
    "          test_data.loc[i,'수술일자'] = 3\n",
    "      \n",
    "      # 위암 병소수 : 병변의 갯수임. 따라서 0은 오타고, 병변의 갯수 만큼(대부분 1) 채워넣으면 됨->  0인 애들,결측치 * 인애들 다 1로 바꿈 \n",
    "      if (test_data.loc[i,'위암병소수'] == '*') or (test_data.loc[i,'위암병소수'] == 0) : \n",
    "          test_data.loc[i,'위암병소수'] = 1\n",
    "      \n",
    "      # '병변1 AGC 분류' 와 '병변1 EGC 분류' 둘 중 하나는 꼭 있어야 되는 feature임. (육안상 진행성 위암인지 조기위암인지) \n",
    "      #   -> 둘다 결측치라면 현미경으로 관측한 정보 'AGC 분류'와 'EGC 분류'로 채워넣음.  \n",
    "      if (test_data.loc[i,'병변1 AGC 분류'] == '*') and (test_data.loc[i,'병변1 EGC 분류'] == '*') :\n",
    "          test_data.loc[i,'병변1 AGC 분류'] = test_data.loc[i,'AGC 분류'] \n",
    "          test_data.loc[i,'병변1 EGC 분류'] = test_data.loc[i,'EGC 분류']\n",
    "\n",
    "      # 'tubular 위치' 컬럼과 '병변1 tubular'의 카테고리를 같게 통일해줌 . (ex. 0: upper)\n",
    "      if (test_data.loc[i,'tubular 위치'] == 1) :\n",
    "          test_data.loc[i,'tubular 위치'] = 0\n",
    "      elif (test_data.loc[i,'tubular 위치'] == 2) :\n",
    "        test_data.loc[i,'tubular 위치'] = 1\n",
    "      elif (test_data.loc[i,'tubular 위치'] == 3) or (test_data.loc[i,'tubular 위치'] == 4) or (test_data.loc[i,'tubular 위치'] == 5) or (test_data.loc[i,'tubular 위치'] == 6) :\n",
    "        test_data.loc[i,'tubular 위치'] = 2\n",
    "      elif (test_data.loc[i,'tubular 위치'] == 7) :\n",
    "        test_data.loc[i,'tubular 위치'] = 3\n",
    "      elif len(str(test_data.loc[i,'tubular 위치'])) != 1 :\n",
    "        test_data.loc[i,'tubular 위치'] = 2 \n",
    "\n",
    "      # 현미경으로 관측한 정보 'tubular 위치'와 'circular 위치'로  '병변1 tubular'와 '병변1 circular'를 채워줌 \n",
    "      if (test_data.loc[i,'병변1 tubular'] == '*') :\n",
    "        test_data.loc[i,'병변1 tubular'] = test_data.loc[i,'tubular 위치']\n",
    "      else : test_data.loc[i,'병변1 tubular'] = int(test_data.loc[i,'병변1 tubular'])\n",
    "      if (test_data.loc[i,'병변1 circular'] == '*') :\n",
    "        test_data.loc[i,'병변1 circular'] = test_data.loc[i,'circular 위치']\n",
    "\n",
    "      # 위에 marginP2,marginD2,marginD3,marginD3는 업데이트 했었음. marginP 업데이트(값 1개만 남게)\n",
    "      if type(test_data.loc[i,'marginP']) == str:\n",
    "        if test_data.loc[i,'marginP'] != '*' :\n",
    "          test_data.loc[i,'marginP']= float(test_data.loc[i,'marginP'].split('/')[0])\n",
    "      if type(test_data.loc[i,'marginD']) == str:\n",
    "        if test_data.loc[i,'marginD'] != '*' :\n",
    "          test_data.loc[i,'marginD']= float(test_data.loc[i,'marginD'].split('/')[0])\n",
    "\n",
    "      # '*'이 결측치가 아니라 '없음'이라는 카테고리인 경울 -> 이미 있는 '없음 카테고리' 0으로 바꿔줌  \n",
    "      if (test_data.loc[i,'가족암병력'] == '*') :\n",
    "        test_data.loc[i,'가족암병력'] = 0\n",
    "      if (test_data.loc[i,'DSL'] == '*') :\n",
    "        test_data.loc[i,'DSL'] = 0   \n",
    "\n",
    "      # 0이 아니라 결측값인 애들 -> '*'로 바꿔줌    \n",
    "      if (test_data.loc[i,'AGC 분류'] == 0) :\n",
    "        test_data.loc[i,'AGC 분류'] = '*'\n",
    "      if (test_data.loc[i,'EGC 분류'] == 0) :\n",
    "        test_data.loc[i,'EGC 분류'] = '*'   \n",
    "\n",
    "  ################# Numerical data #################\n",
    "  # Numerical data imputation -> 평균으로 대체\n",
    "  Ordinal_data = ['ASA score','LN dissection ','위암병소수' ] \n",
    "  Numerical_data = ['나이','신장(cm)', '체중(kg)','BMI','수술시간(min)','출혈량(mL)','GAS OUT','SD start',\n",
    "                    '병변1 크기(Cm)','marginP','marginD','구득 림프절수','전이 림프절수'] +Ordinal_data\n",
    "\n",
    "  for numeric in Numerical_data :\n",
    "    test_data[numeric] =  test_data[numeric].replace('*',np.NaN)\n",
    "    test_data[numeric] = pd.to_numeric(test_data[numeric])\n",
    "    \n",
    "    test_data[numeric]= test_data[numeric].fillna(train_nu_mean)\n",
    "  ## bmi 는 (체중*10000) / (신장**2)\n",
    "  test_data['BMI'] = (test_data['체중(kg)']*10000)/(test_data['신장(cm)']**2)\n",
    "\n",
    "  # numerical data 인데*이 결측치가 아니라 측정 안한거: '*'을 0으로 바꾸면 됨. \n",
    "\n",
    "  star_is_zero_numeric = [ '병변2 크기(Cm)', '병변3 크기(Cm)','marginP2','marginD2','marginP3','marginD3',\n",
    "                              'WBC_Pre', 'WBC_Post', 'WBC_POD#1', 'WBC_POD#2', 'WBC_POD#3', 'WBC_POD#5', 'WBC_POD#7',\n",
    "                              'Hb_Pre', 'Hb_Post', 'Hb_POD#1', 'Hb_POD#2', 'Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7', \n",
    "                              'AST_Pre', 'AST_Post', 'AST_POD#1', 'AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7', \n",
    "                              'ALT_Pre', 'ALT_Post', 'ALT_POD#1', 'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5', 'ALT_POD#7', \n",
    "                              'CRP_Pre', 'CRP_Post', 'CRP_POD#1', 'CRP_POD#2', 'CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "                              'JP_Amy_Rt_POD#1', 'JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5', \n",
    "                              'JP_Rt_color change', 'JP_Lip_Rt_POD#1', 'JP_Lip_Rt_POD#2', 'JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "                              'JP_Amy_Lt_POD#1', 'JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5', \n",
    "                              'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']\n",
    "  for i in range(len(test_data)):\n",
    "    for value in star_is_zero_numeric :\n",
    "      if (test_data.loc[i,value] == '*') :\n",
    "        test_data.loc[i,value] = 0      \n",
    "\n",
    "  ################# Categorical data #################\n",
    "\n",
    "  ## '*'을 그대로 놔둬야 하는 애들 -> '*'이라는 category를 주고 imputation 안함. \n",
    "  star_is_category = ['AGC 분류','EGC 분류','tubular 위치','circular 위치','병변1 AGC 분류','병변1 EGC 분류',\n",
    "                  '병변1 tubular', '병변1 circular','병변2 AGC 분류','병변2 EGC 분류',\n",
    "                  '병변2 tubular', '병변2 circular','병변3 AGC 분류','병변3 EGC 분류',\n",
    "                  '병변3 tubular', '병변3 circular' ]\n",
    "\n",
    "\n",
    "  ## imputation 해야 되는 애들 \n",
    "\n",
    "  ###  결측치가 10 % 미만인 컬럼들 -> 빈도수 가장 많은 걸로\n",
    "  test_data = test_data.rename(columns={'ESD/EMR': 'ESDEMR','정규/응급': '정규응급'})\n",
    "  Categorical_data_under10 =['성별','수술일자','smoking','가족암병력','HTN','DM','Dyslipidemia',\n",
    "                      '심혈관질환','뇌혈관질환','신장질환','호흡기질환','기타 기저질환','복부 수술력','복부수술력 중 위관련',\n",
    "                      '기타 수술력','수술 전   심초음파','수술 전   폐기능검사','수술 전    흉부CT검사',\n",
    "                      'ESDEMR','정규응급','수술명','개복전환','문합법','합병절제                   ',\n",
    "                      'adhesion','invasion','radicality','혈관변이',\n",
    "                      '수혈']\n",
    "  for i in range(len(test_data)):\n",
    "    for j,bin in enumerate(Categorical_data_under10):\n",
    "      if test_data.loc[i,bin] == '*' :\n",
    "          test_data.loc[i,bin] = binbin[j]\n",
    "\n",
    "  ### 결측치가 10 % 이상인 컬럼들 -> random sample imputation :원래의 분포를 유지하도록 결측치를 imputation  \n",
    "  Categorical_data_over10 =['depth','Stage','WHO classification','WHO 세포 분화도',\"Lauren's classification\",\n",
    "                          'Ming classification','lymphatics invasion','vascular invasion','perineural invasion',\n",
    "                          'additional findings']    \n",
    "  for category10 in Categorical_data_over10 :\n",
    "    test_data['_random'] = test_data[category10]\n",
    "    test_data.loc[test_data[category10]!='*'][category10]\n",
    "    \n",
    "    test_data.loc[(test_data[category10]=='*'), '_random'] = temp\n",
    "    test_data[category10] = test_data['_random']\n",
    "    test_data=test_data.drop(['_random'],axis =1 )\n",
    "\n",
    "  test_data = test_data.fillna(0)\n",
    "\n",
    "  ## Onehotencoding\n",
    "  CATEGROCAL = star_is_category+Categorical_data_under10+Categorical_data_over10\n",
    "  test_data[CATEGROCAL]= test_data[CATEGROCAL].astype(str)\n",
    "  test_data= pd.get_dummies(test_data, columns = CATEGROCAL)\n",
    "\n",
    "  # EGC 분류,circular 위치,가족암병력, WHO classification, WHO 세포분화도,Lauren's classification,Ming classification 처럼\n",
    "  # 한 사람이 값을 2개 씩 가질 수 있는 컬럼 -> 원핫 인코딩하면 '가족암병력_2,3' 이런 컬럼이 추가됨. 이거 다시 나눠줌.  \n",
    "  remo =[]\n",
    "  for i in range(len(test_data)):\n",
    "    for col in test_data.columns:\n",
    "      if (',' in col) :\n",
    "        remo.append(col)\n",
    "        if (test_data.loc[i,col] == 1) :\n",
    "          splitted_col = col.split(',')\n",
    "          test_data.loc[i,splitted_col[0]] = 1\n",
    "          for j in range(1,len(splitted_col)):\n",
    "            test_data.loc[i,(splitted_col[0].split('_'))[0]+'_'+splitted_col[j] ] = 1\n",
    "      if ('/' in col) :\n",
    "        remo.append(col)\n",
    "        if (test_data.loc[i,col] == 1) :\n",
    "          splitted_col = col.split('/')\n",
    "          test_data.loc[i,splitted_col[0]] = 1     \n",
    "          for j in range(1,len(splitted_col)):\n",
    "            test_data.loc[i,(splitted_col[0].split('_'))[0]+'_'+splitted_col[j] ] = 1\n",
    "  a = list(set(remo))\n",
    "  test_data= test_data.drop(a,axis=1)    \n",
    "\n",
    "  test_data = test_data.fillna(0)\n",
    "  test_data = test_data.astype('float')\n",
    "\n",
    "  return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1668788684564,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "dvBpnykcpx1S"
   },
   "outputs": [],
   "source": [
    "mask = 0\n",
    "def columns_process(df1,df2):\n",
    "  \n",
    "  train_col = df1.columns\n",
    "  test_col = df2.columns\n",
    "\n",
    "  # train에 없는 컬럼\n",
    "  train_no =[]\n",
    "  for col in test_col :\n",
    "    if col not in train_col :\n",
    "      train_no.append(col)\n",
    "\n",
    "  # test에 없는 컬럼    \n",
    "  test_no =[]\n",
    "  for col in train_col :\n",
    "    if col not in test_col :\n",
    "      test_no.append(col)\n",
    "\n",
    "  df1[train_no]= mask\n",
    "  df2[test_no] =mask\n",
    "\n",
    "  df2 = df2[df1.columns]\n",
    "  return df1, df2\n",
    "\n",
    "def compute_ndcg(y_prob, y_true, k=10):\n",
    "    # K=10, K=50, K=100\n",
    "    assert len(y_prob) >= k\n",
    "    assert len(y_true) >= k\n",
    "\n",
    "    relevance = y_true == 1\n",
    "    n_target = relevance.sum()\n",
    "    rank = np.argsort(y_prob)[::-1]\n",
    "\n",
    "    dcg = [relevance[rank[i]] / (math.log2(i + 2)) for i in range(k)]\n",
    "    idcg = [(i < n_target) / (math.log2(i + 2)) for i in range(k)]\n",
    "\n",
    "    return sum(dcg) / sum(idcg)\n",
    "\n",
    "def prediction_day_augmentation(DATA) : \n",
    "  train_0 =DATA.copy()\n",
    "  \n",
    "  train_0['Prediction day'] = 0\n",
    "  \n",
    "  train_0[['WBC_POD#1', 'WBC_POD#2', 'WBC_POD#3','WBC_POD#5', 'WBC_POD#7', \n",
    "          'Hb_POD#1', 'Hb_POD#2','Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7', \n",
    "          'AST_POD#1','AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "        'ALT_POD#1', 'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5','ALT_POD#7', \n",
    "        'CRP_POD#1', 'CRP_POD#2','CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "        'JP_Amy_Rt_POD#1','JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "        'JP_Rt_color change', \n",
    "        'JP_Lip_Rt_POD#1', 'JP_Lip_Rt_POD#2','JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "        'JP_Amy_Lt_POD#1','JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "        'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] = mask\n",
    "  \n",
    "  train_1 =DATA.copy()\n",
    "  \n",
    "  train_1['Prediction day'] = 1\n",
    "  train_1[['WBC_POD#2', 'WBC_POD#3','WBC_POD#5', 'WBC_POD#7', \n",
    "          'Hb_POD#2','Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7', \n",
    "          'AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "        'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5','ALT_POD#7', \n",
    "        'CRP_POD#2','CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "        'JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "        \n",
    "        'JP_Lip_Rt_POD#2','JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "        'JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "        'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] =mask\n",
    "\n",
    "  train_2 =DATA.copy()\n",
    "  \n",
    "  train_2['Prediction day'] = 2\n",
    "  train_2[[ 'WBC_POD#3','WBC_POD#5', 'WBC_POD#7', \n",
    "        'Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7', \n",
    "          'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "          'ALT_POD#3', 'ALT_POD#5','ALT_POD#7',  \n",
    "        'CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "          'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "\n",
    "        'JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5',\n",
    "        'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "        'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] = mask\n",
    "  train_3 =DATA.copy()\n",
    "  \n",
    "  train_3['Prediction day'] = 3\n",
    "  train_3[[ 'WBC_POD#5', 'WBC_POD#7', \n",
    "          'Hb_POD#5', 'Hb_POD#7', \n",
    "          'AST_POD#5', 'AST_POD#7',\n",
    "          'ALT_POD#5','ALT_POD#7',  \n",
    "          'CRP_POD#5', 'CRP_POD#7', \n",
    "          'JP_Amy_Rt_POD#5',\n",
    "        'JP_Rt_color change',\n",
    "        'JP_Lip_Rt_POD#5',\n",
    "        'JP_Amy_Lt_POD#5',\n",
    "        'JP_Lip_Lt_POD#5']] = mask\n",
    "  train_5 =DATA.copy()\n",
    "  \n",
    "  train_5['Prediction day'] = 5\n",
    "  train_5[[  'WBC_POD#7', \n",
    "          'Hb_POD#7', \n",
    "          'AST_POD#7',\n",
    "          'ALT_POD#7',  \n",
    "          'CRP_POD#7']] = mask\n",
    "  for i in range(len(train_5)):\n",
    "    \n",
    "    if train_5.loc[i,'JP_Rt_color change'] == 6 :\n",
    "\n",
    "      train_5.loc[i,'JP_Rt_color change'] = mask\n",
    "  \n",
    "  train_7 =DATA.copy()\n",
    "  \n",
    "  train_7['Prediction day'] = 7\n",
    "  \n",
    "  DATA_AUG = pd.concat([train_0,train_1,train_2,train_3,train_5,train_7],ignore_index=True)\n",
    "  \n",
    "  return DATA_AUG\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = list(y_train)\n",
    "        self.length = len(X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor(np.array(self.X_train[index]))\n",
    "        y = torch.FloatTensor(np.array(self.y_train[index]))\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(X_val.shape[1], 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_3 = nn.Linear(64, 32)\n",
    "        self.layer_out = nn.Linear(32, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(32)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(self.layer_3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        x= self.Sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1668788684566,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "5FuFc_BfAx8u"
   },
   "outputs": [],
   "source": [
    "def basic_test(k) :\n",
    "  gb = GradientBoostingClassifier(random_state =0)\n",
    "  mlp = MLPClassifier(hidden_layer_sizes=(100,100), \n",
    "                        activation='tanh', solver='lbfgs',\n",
    "                        batch_size='auto', learning_rate='adaptive', alpha=0.0001,max_iter=1000)\n",
    "  svm_model =svm.SVC(kernel = 'rbf',C=8,gamma =0.1)\n",
    "  rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "  xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1,max_depth=3)\n",
    "\n",
    "  mlpp = MLPClassifier(hidden_layer_sizes=(10,10), activation='relu',solver='adam', \n",
    "                      alpha=0.01, batch_size=64,learning_rate_init=0.01, max_iter=100)\n",
    "\n",
    "  clf = BaggingClassifier(base_estimator=mlpp, n_estimators=30, random_state=0)\n",
    "\n",
    "  #models = [gb,mlp,svm_model,rf,xgb_wrapper,clf]\n",
    "  #model_name = [\"gb\",\"mlp\",\"svm_model\",\"rf\",\"xgb_wrapper\",\"clf\"]\n",
    "  models = [gb,mlp,svm_model,rf,xgb_wrapper]\n",
    "  model_name = [\"gb\",\"mlp\",\"svm_model\",\"rf\",\"xgb_wrapper\"]\n",
    "\n",
    "  for i in range(len(models)):\n",
    "    models[i].fit(X_train,y_train)\n",
    "    y_pred = models[i].predict(X_val)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(model_name[i],\": {:.5f}\".format(np.mean(y_pred == y_val))) # 예측 정확도\n",
    "    cf = confusion_matrix(y_val, y_pred)\n",
    "    print(cf)\n",
    "    print(\"---------\")\n",
    "    joblib.dump(models[i], model_name[i]+f'_{k}.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onset_masking(data_not_masked) :\n",
    "    data=data_not_masked.copy()\n",
    "    train_data_onset_masking=train_data.drop(['JP_Lt_color change','입원일자','퇴원일자','Patient number','타장기원발암','Clipping ',\n",
    "                                    '수술 전 EUS','수술 전 PET-CT','TNM','CEA(ng/mL)','CA19-9(U/mL)','cTNM',\n",
    "                                    'post ESD'],axis =1 )  \n",
    "    train_data_onset_masking = train_data_onset_masking.loc[(train_data_onset_masking =='*').mean(axis=1)<0.7]\n",
    "    train_data_onset_masking=train_data_onset_masking.reset_index(drop=True)\n",
    "    list_a =[0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    for i in range(len(train_data_onset_masking)):\n",
    "        if train_data_onset_masking.loc[i,'onset']== '*':\n",
    "            train_data_onset_masking.loc[i,'onset']=random.choice(list_a)\n",
    "    train_data_onset_masking['Prediction day']= train_data_onset_masking['onset']-1\n",
    "\n",
    "    data['Prediction day']=train_data_onset_masking['Prediction day']\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if data.loc[i,'Prediction day'] <=0:\n",
    "            data.loc[i,'Prediction day'] =0\n",
    "            data.loc[i,['WBC_POD#1', 'WBC_POD#2', 'WBC_POD#3','WBC_POD#5', 'WBC_POD#7',\n",
    "                              'Hb_POD#1', 'Hb_POD#2','Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7',\n",
    "                              'AST_POD#1','AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "                              'ALT_POD#1', 'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5','ALT_POD#7', \n",
    "                              'CRP_POD#1', 'CRP_POD#2','CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "                              'JP_Amy_Rt_POD#1','JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "                              'JP_Lip_Rt_POD#1', 'JP_Lip_Rt_POD#2','JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "                              'JP_Amy_Lt_POD#1','JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "                              'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] = 0\n",
    "\n",
    "        elif data.loc[i,'Prediction day'] == 1:\n",
    "            data.loc[i,['WBC_POD#2', 'WBC_POD#3','WBC_POD#5', 'WBC_POD#7',\n",
    "                              'Hb_POD#2','Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7',\n",
    "                              'AST_POD#2', 'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "                              'ALT_POD#2', 'ALT_POD#3', 'ALT_POD#5','ALT_POD#7', \n",
    "                              'CRP_POD#2','CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "                              'JP_Amy_Rt_POD#2', 'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "                              'JP_Lip_Rt_POD#2','JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "                              'JP_Amy_Lt_POD#2', 'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "                              'JP_Lip_Lt_POD#2', 'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] = 0\n",
    "        elif data.loc[i,'Prediction day'] == 2 :\n",
    "            data.loc[i,['WBC_POD#3','WBC_POD#5', 'WBC_POD#7',\n",
    "                              'Hb_POD#3', 'Hb_POD#5', 'Hb_POD#7',\n",
    "                              'AST_POD#3', 'AST_POD#5', 'AST_POD#7',\n",
    "                              'ALT_POD#3', 'ALT_POD#5','ALT_POD#7', \n",
    "                              'CRP_POD#3', 'CRP_POD#5', 'CRP_POD#7', \n",
    "                              'JP_Amy_Rt_POD#3', 'JP_Amy_Rt_POD#5',\n",
    "                              'JP_Lip_Rt_POD#3', 'JP_Lip_Rt_POD#5', \n",
    "                              'JP_Amy_Lt_POD#3', 'JP_Amy_Lt_POD#5',\n",
    "                              'JP_Lip_Lt_POD#3', 'JP_Lip_Lt_POD#5']] = 0\n",
    "        elif (data.loc[i,'Prediction day'] == 3) or (data.loc[i,'Prediction day'] == 4):\n",
    "            data.loc[i,['WBC_POD#5', 'WBC_POD#7',\n",
    "                              'Hb_POD#5', 'Hb_POD#7',\n",
    "                              'AST_POD#5', 'AST_POD#7',\n",
    "                              'ALT_POD#5','ALT_POD#7', \n",
    "                              'CRP_POD#5', 'CRP_POD#7', \n",
    "                              'JP_Amy_Rt_POD#5',\n",
    "                              'JP_Lip_Rt_POD#5', \n",
    "                              'JP_Amy_Lt_POD#5',\n",
    "                              'JP_Lip_Lt_POD#5']] = 0\n",
    "\n",
    "        elif (data.loc[i,'Prediction day'] == 5) or (data.loc[i,'Prediction day'] == 6):\n",
    "            data.loc[i,['WBC_POD#7',\n",
    "                              'Hb_POD#7',\n",
    "                              'AST_POD#7',\n",
    "                              'ALT_POD#7', \n",
    "                              'CRP_POD#7']] = 0\n",
    "        elif data.loc[i,'Prediction day'] >=7:\n",
    "            data.loc[i,'Prediction day'] =7\n",
    "\n",
    "        if data.loc[i,'JP_Rt_color change'] == 4 : \n",
    "            if (data.loc[i,'Prediction day'] < 4) :\n",
    "              data.loc[i,'JP_Rt_color change'] = 0\n",
    "        elif data.loc[i,'JP_Rt_color change'] == 6 : \n",
    "            if (data.loc[i,'Prediction day'] < 6) :\n",
    "              data.loc[i,'JP_Rt_color change'] = 0\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -fold\n",
      " \n",
      "gb : 0.97222\n",
      "[[105   0]\n",
      " [  3   0]]\n",
      "---------\n",
      " \n",
      "mlp : 0.97222\n",
      "[[105   0]\n",
      " [  3   0]]\n",
      "---------\n",
      " \n",
      "svm_model : 0.97222\n",
      "[[105   0]\n",
      " [  3   0]]\n",
      "---------\n",
      " \n",
      "rf : 0.97222\n",
      "[[105   0]\n",
      " [  3   0]]\n",
      "---------\n",
      " \n",
      "xgb_wrapper : 0.97222\n",
      "[[105   0]\n",
      " [  3   0]]\n",
      "---------\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Songwooseok\\miniconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n",
      "torch.Size([64, 307])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13924\\682845748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[0mbasic_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m   \u001b[0my_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompute_ndcg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13924\\682845748.py\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m           \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m           \u001b[1;31m#print(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m           \u001b[1;31m#print(output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13924\\2100274878.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#######################\n",
    "def train_test(k):\n",
    "  train_dataset = CustomDataset()\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  model = BinaryClassification().to(device)\n",
    "  criterion = nn.BCELoss().to(device)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "  model.train()\n",
    "  cost_list = []\n",
    "  ppp=0\n",
    "  best_acc =0\n",
    "  best_score = -1\n",
    "  min_cost = 100\n",
    "  for epoch in range(2000):\n",
    "      cost = 0.0\n",
    "      \n",
    "      for x, y in train_dataloader:\n",
    "          print(x.shape)\n",
    "          #print(y.shape)\n",
    "          x = x.squeeze().to(device)\n",
    "          #print(\"ddd\",x.shape)\n",
    "          y = y.to(device)\n",
    "\n",
    "          output = model(x).squeeze()\n",
    "          #print(output)\n",
    "          #print(output.shape)\n",
    "          #print(y.shape)\n",
    "          loss = criterion(output, y.squeeze())\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          #print(\"dd\")\n",
    "          cost += loss\n",
    "\n",
    "      cost = cost / len(train_dataloader)\n",
    "      if (epoch + 1) % 100 == 0:\n",
    "          print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          model.eval()\n",
    "          inputs = torch.FloatTensor(\n",
    "              [np.array(X_val)]\n",
    "          ).to(device)\n",
    "\n",
    "          outputs = model(inputs.squeeze()).squeeze()\n",
    "      \n",
    "      acc  = np.mean(list(torch.round(outputs)) == y_val)\n",
    "      score = sum([compute_ndcg(np.array(outputs), y_val, i) for i in [10, 50, 100]])\n",
    "      \n",
    "      if (score > best_score) and (min_cost >cost) and (best_acc < acc ):\n",
    "            best_score = score\n",
    "            min_cost =cost\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), f'Newfold_{k}.pt')\n",
    "  model = BinaryClassification()\n",
    "  path = f'Newfold_{k}.pt'\n",
    "  model.load_state_dict(torch.load(path))\n",
    "  with torch.no_grad():\n",
    "          model.eval()\n",
    "          inputs = torch.FloatTensor(\n",
    "              [np.array(X_val)]\n",
    "          ).to(device)\n",
    "\n",
    "          outputs = model(inputs.squeeze()).squeeze()          \n",
    "  cf = confusion_matrix(y_val, list(torch.round(outputs)))\n",
    "\n",
    "  print(\"MY MODEL accuracy: {:.5f}\".format(np.mean(list(torch.round(outputs)) == y_val))) # 예측 정확도\n",
    "  print(cf)\n",
    "  \n",
    "  return outputs\n",
    "\n",
    "train_data_origin,temp,train_nu_mean,binbin= pre_processing(train_data)\n",
    "test_data = preprocess_test(test_data)\n",
    "\n",
    "train_data_origin, test_data = columns_process(train_data_origin,test_data)\n",
    "\n",
    "SEED = 14\n",
    "data = train_data_origin.drop(\"DSL\", axis=1)\n",
    "y = train_data_origin['DSL']\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "y_prob_list =[]\n",
    "y_val_list =[]\n",
    "scores =[]\n",
    "for k, (train_index, val_index) in enumerate(skf.split(data, y)): # 5번\n",
    "  print(k,\"-fold\")\n",
    "  X_train = prediction_day_augmentation(train_data_origin.iloc[train_index,:].reset_index(drop= True))\n",
    "  y_train = X_train[\"DSL\"]\n",
    "  X_train = X_train.drop(\"DSL\",axis=1)\n",
    "\n",
    "  X_val = onset_masking(train_data_origin).iloc[val_index,:].reset_index(drop= True)\n",
    "  y_val = X_val[\"DSL\"]\n",
    "  X_val = X_val.drop(\"DSL\",axis=1)\n",
    "\n",
    "  test_data_xdsl = test_data.drop(\"DSL\", axis=1)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_val  = scaler.transform(X_val)\n",
    "  X_test  = scaler.transform(test_data_xdsl)\n",
    "  \n",
    "  basic_test(k)\n",
    "  \n",
    "  y_prob = train_test(k)\n",
    "\n",
    "  score = sum([compute_ndcg(np.array(y_prob), y_val, i) for i in [10, 50, 100]])\n",
    "  scores.append(score)\n",
    "  #print(score)\n",
    "  print(\"-------------------------------------------------\")\n",
    "  print(\" \")\n",
    "  y_prob_list.append(np.array(y_prob))\n",
    "  y_val_list.append(y_val)\n",
    "\n",
    "print(\"MY MODEL 최종 SCORE는\" ,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_0 = np.array(pd.DataFrame({'y_true' : np.array(y_val_list[0]),'y_prob' : y_prob_list[0]})).tolist()\n",
    "fold_1 = np.array(pd.DataFrame({'y_true' : np.array(y_val_list[1]),'y_prob' : y_prob_list[1]})).tolist()\n",
    "fold_2 = np.array(pd.DataFrame({'y_true' : np.array(y_val_list[2]),'y_prob' : y_prob_list[2]})).tolist()\n",
    "fold_3 = np.array(pd.DataFrame({'y_true' : np.array(y_val_list[3]),'y_prob' : y_prob_list[3]})).tolist()\n",
    "fold_4 = np.array(pd.DataFrame({'y_true' : np.array(y_val_list[4]),'y_prob' : y_prob_list[4]})).tolist()\n",
    "save_filename = 'fold_0.txt'\n",
    "www = open(save_filename, 'w')\n",
    "for k in fold_0 :\n",
    "    www.write(str(k).strip(\"[\"\"]\") + '\\n')\n",
    "www.close()\n",
    "\n",
    "save_filename = 'fold_1.txt'\n",
    "www = open(save_filename, 'w')\n",
    "for k in fold_1 :\n",
    "    www.write(str(k).strip(\"[\"\"]\") + '\\n')\n",
    "www.close()\n",
    "\n",
    "save_filename = 'fold_2.txt'\n",
    "www = open(save_filename, 'w')\n",
    "for k in fold_2 :\n",
    "    www.write(str(k).strip(\"[\"\"]\") + '\\n')\n",
    "www.close()\n",
    "\n",
    "save_filename = 'fold_3.txt'\n",
    "www = open(save_filename, 'w')\n",
    "for k in fold_3 :\n",
    "    www.write(str(k).strip(\"[\"\"]\") + '\\n')\n",
    "www.close()\n",
    "\n",
    "save_filename = 'fold_4.txt'\n",
    "www = open(save_filename, 'w')\n",
    "for k in fold_4 :\n",
    "    www.write(str(k).strip(\"[\"\"]\") + '\\n')\n",
    "www.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "error",
     "timestamp": 1668829453228,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "23Z_zSIGpT9R",
    "outputId": "562711ad-67b6-4160-fb02-10ef08632e26",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/content/mlp_4.pkl')\n",
    "for k, (train_index, val_index) in enumerate(skf.split(data, y)): # 5번\n",
    "  print(k,\"-fold\")\n",
    "  X_train = prediction_day_augmentation(train_data_origin.iloc[train_index,:].reset_index(drop= True))\n",
    "  y_train = X_train[\"DSL\"]\n",
    "  X_train = X_train.drop(\"DSL\",axis=1)\n",
    "\n",
    "  X_val = train_data_origin.iloc[val_index,:].reset_index(drop= True)\n",
    "  y_val = X_val[\"DSL\"]\n",
    "  X_val = X_val.drop(\"DSL\",axis=1)\n",
    "\n",
    "  test_data_xdsl = test_data.drop(\"DSL\", axis=1)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_val  = scaler.transform(X_val)\n",
    "  X_test  = scaler.transform(test_data_xdsl)\n",
    "\n",
    "  y_pred = model.predict(X_val)\n",
    "  y_prob = model.predict_proba(X_val)[:,1]\n",
    "  \n",
    "  cf = confusion_matrix(y_val, y_pred)\n",
    "  print(cf)\n",
    "  score = sum([compute_ndcg(np.array(y_prob), y_val, i) for i in [10, 50, 100]])\n",
    "  print(score)\n",
    "  \n",
    "  print(\"-------------------------------------------------\")\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UjdNKvbUT-w"
   },
   "source": [
    "모델 언제까지 학습해 언제저장해 val로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1668790611218,
     "user": {
      "displayName": "송우석",
      "userId": "17544301414917834266"
     },
     "user_tz": -540
    },
    "id": "3J5HPJcUpsoH",
    "outputId": "7f0c5c40-01b7-48dd-a19e-c5a9f0b0fbe9"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiNUNgIIq-ls"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOlc1alnoFHvNSWzOiiK5C6",
   "mount_file_id": "14hpNhnnUly6mq9fGrxv-xUeNFFhM5qbV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
